{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torchvision\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "# check device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else device)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# prepare dataset\n",
    "trainDataset = torchvision.datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.ToTensor()\n",
    ")\n",
    "\n",
    "testDataset = torchvision.datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.ToTensor()\n",
    ")\n",
    "\n",
    "# Confirm they are in grayscale (they should already be).\n",
    "# should be [1, 28, 28], 1 channel, 28x28 pixels (grayscale)\n",
    "print(trainDataset[0][0].shape)\n",
    "print(testDataset[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def siftFeatureDetector(image):\n",
    "    sift = cv2.SIFT_create()\n",
    "    image = (image * 255).astype(np.uint8)\n",
    "    keypoints, descriptors = sift.detectAndCompute(image, None)\n",
    "    return keypoints, descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraDescriptorsAndLabels(dataSet, batch_size=128):\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataSet, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    all_descriptors = []\n",
    "    all_labels = []\n",
    "\n",
    "    # use tqdm to show progress bar\n",
    "    for batch in tqdm(train_loader, desc=\"Processing Batches\", unit=\"batch\"):\n",
    "        images, labels = batch\n",
    "        images = images.squeeze().numpy()\n",
    "        for image in images:\n",
    "            keypoints, descriptors = siftFeatureDetector(image)\n",
    "            all_descriptors.append(descriptors)\n",
    "            all_labels.append(labels)\n",
    "    # verify that the descriptors and the labels are being stored correctly\n",
    "    if len(all_descriptors) == len(all_labels) and len(all_descriptors) == len(dataSet):\n",
    "        return all_descriptors, all_labels\n",
    "    else:\n",
    "        print(\"Error: descriptors and labels are not the same length\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 469/469 [00:19<00:00, 24.41batch/s]\n",
      "Processing Batches: 100%|██████████| 79/79 [00:03<00:00, 24.65batch/s]\n"
     ]
    }
   ],
   "source": [
    "# get descriptors and labels\n",
    "train_descriptors, train_labels = extraDescriptorsAndLabels(trainDataset)\n",
    "test_descriptors, test_labels = extraDescriptorsAndLabels(testDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task1_MLP(nn.Module):\n",
    "    def __init__(self, input_dim=28 * 28, hidden_dim=128, output_dim=10):\n",
    "        super(Task1_MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)  # 输入层: 50 -> 128\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)  # 隐藏层: 128 -> 10（类别数）\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/469], Loss: 0.6045718193054199\n",
      "Epoch [1/5], Step [200/469], Loss: 0.4898671507835388\n",
      "Epoch [1/5], Step [300/469], Loss: 0.4991433620452881\n",
      "Epoch [1/5], Step [400/469], Loss: 0.4031980633735657\n",
      "Epoch [2/5], Step [100/469], Loss: 0.38540399074554443\n",
      "Epoch [2/5], Step [200/469], Loss: 0.4107586145401001\n",
      "Epoch [2/5], Step [300/469], Loss: 0.44177696108818054\n",
      "Epoch [2/5], Step [400/469], Loss: 0.43348944187164307\n",
      "Epoch [3/5], Step [100/469], Loss: 0.48607054352760315\n",
      "Epoch [3/5], Step [200/469], Loss: 0.44146448373794556\n",
      "Epoch [3/5], Step [300/469], Loss: 0.2834351062774658\n",
      "Epoch [3/5], Step [400/469], Loss: 0.3967074751853943\n",
      "Epoch [4/5], Step [100/469], Loss: 0.4138443171977997\n",
      "Epoch [4/5], Step [200/469], Loss: 0.2710345387458801\n",
      "Epoch [4/5], Step [300/469], Loss: 0.4316185712814331\n",
      "Epoch [4/5], Step [400/469], Loss: 0.34148502349853516\n",
      "Epoch [5/5], Step [100/469], Loss: 0.5268257856369019\n",
      "Epoch [5/5], Step [200/469], Loss: 0.23402893543243408\n",
      "Epoch [5/5], Step [300/469], Loss: 0.38127395510673523\n",
      "Epoch [5/5], Step [400/469], Loss: 0.280972957611084\n"
     ]
    }
   ],
   "source": [
    "# convert 28*28 image to 784-dim vector\n",
    "def imageToVector(image):\n",
    "    return image.view(-1, 28 * 28)\n",
    "\n",
    "# train the model\n",
    "model = Task1_MLP().to(device)\n",
    "\n",
    "# define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# train the model\n",
    "num_epochs = 5\n",
    "batch_size = 128\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    trainDataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(imageToVector(images))\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
